{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee985ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from scipy.stats import mode\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess Data\n",
    "print(\"ğŸ”„ Loading dataset...\")\n",
    "students_df = pd.read_csv(\"../datasets/dataset.csv\")\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "Q1 = students_df.select_dtypes(include=[np.number]).quantile(0.25)\n",
    "Q3 = students_df.select_dtypes(include=[np.number]).quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = (\n",
    "    (students_df.select_dtypes(include=[np.number]) < lower_bound)\n",
    "    | (students_df.select_dtypes(include=[np.number]) > upper_bound)\n",
    ").any(axis=1)\n",
    "students_df_cleaned = students_df[~outliers].copy()\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = students_df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "students_df_cleaned[numerical_cols] = scaler.fit_transform(\n",
    "    students_df_cleaned[numerical_cols]\n",
    ")\n",
    "\n",
    "students_df_normalised_no_outliers = students_df_cleaned.copy()\n",
    "\n",
    "print(f\"âœ“ Data preprocessed: {len(students_df_normalised_no_outliers)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54074d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Features and Target\n",
    "print(\"ğŸ“Š Preparing features and target...\")\n",
    "\n",
    "# Convert Target to numerical\n",
    "students_df_normalised_no_outliers[\"Target\"] = students_df_normalised_no_outliers[\n",
    "    \"Target\"\n",
    "].replace({\"Dropout\": 0, \"Graduate\": 1, \"Enrolled\": 2})\n",
    "\n",
    "# Remove enrolled students\n",
    "students_df_normalised_no_outliers = students_df_normalised_no_outliers[\n",
    "    students_df_normalised_no_outliers[\"Target\"] != 2\n",
    "]\n",
    "\n",
    "# Remove features based on notebook analysis\n",
    "features_to_remove = [\n",
    "    \"Curricular units 1st sem (credited)\",\n",
    "    \"Curricular units 1st sem (enrolled)\",\n",
    "    \"Curricular units 1st sem (evaluations)\",\n",
    "    \"Curricular units 1st sem (approved)\",\n",
    "    \"Curricular units 1st sem (grade)\",\n",
    "    \"Curricular units 2nd sem (approved)\",\n",
    "    \"Nationality\",\n",
    "]\n",
    "\n",
    "students_df_normalised_no_outliers = students_df_normalised_no_outliers.drop(\n",
    "    features_to_remove, axis=1, errors=\"ignore\"\n",
    ")\n",
    "\n",
    "# Prepare X and y\n",
    "X = students_df_normalised_no_outliers.drop(\"Target\", axis=1)\n",
    "y = students_df_normalised_no_outliers[\"Target\"]\n",
    "\n",
    "print(f\"âœ“ Features: {X.shape[1]}\")\n",
    "print(f\"âœ“ Samples: {X.shape[0]}\")\n",
    "print(f\"âœ“ Class distribution: {dict(y.value_counts())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Random Oversampling\n",
    "print(\"ğŸ”„ Applying Random Oversampling...\")\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "print(f\"âœ“ Resampled: {X_resampled.shape[0]} samples\")\n",
    "print(f\"âœ“ Balanced classes: {dict(pd.Series(y_resampled).value_counts())}\")\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "results_comparison = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b8b86",
   "metadata": {},
   "source": [
    "## 1. Random Forest Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1776471e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Tuning completed in 7970.00s\n",
      "âœ“ Best Parameters: {'bootstrap': False, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "âœ“ Best CV Score: 0.9816\n",
      "ğŸ“ˆ Improvement: +1.91%\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸŒ²\" * 40)\n",
    "print(\"1. RANDOM FOREST CLASSIFIER - ADVANCED TUNING\")\n",
    "print(\"ğŸŒ²\" * 40)\n",
    "\n",
    "# Default performance\n",
    "print(\"\\nğŸ“Œ Default Parameters Performance:\")\n",
    "rf_default = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_default_start = time.time()\n",
    "rf_default_scores = cross_val_score(\n",
    "    rf_default, X_resampled, y_resampled, cv=cv, scoring=\"accuracy\"\n",
    ")\n",
    "rf_default_time = time.time() - rf_default_start\n",
    "rf_default_mean = rf_default_scores.mean()\n",
    "\n",
    "print(f\"   Accuracy: {rf_default_mean:.4f} (+/- {rf_default_scores.std():.4f})\")\n",
    "print(f\"   Training Time: {rf_default_time:.2f}s\")\n",
    "\n",
    "# Advanced Tuning - Expanded Grid\n",
    "print(\"\\nğŸ”§ Advanced Hyperparameter Tuning...\")\n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300, 400],\n",
    "    \"max_depth\": [15, 20, 25, 30, None],\n",
    "    \"min_samples_split\": [2, 3, 5, 7],\n",
    "    \"min_samples_leaf\": [1, 2, 3, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", 0.5, 0.7],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"bootstrap\": [True, False],\n",
    "}\n",
    "\n",
    "print(f\"   Total combinations: {4*5*4*4*4*2*2} = 5,120\")\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    rf_param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "rf_tuning_start = time.time()\n",
    "rf_grid.fit(X_resampled, y_resampled)\n",
    "rf_tuning_time = time.time() - rf_tuning_start\n",
    "\n",
    "print(f\"\\nâœ“ Tuning completed in {rf_tuning_time:.2f}s\")\n",
    "print(f\"âœ“ Best Parameters: {rf_grid.best_params_}\")\n",
    "print(f\"âœ“ Best CV Score: {rf_grid.best_score_:.4f}\")\n",
    "\n",
    "rf_improvement = ((rf_grid.best_score_ - rf_default_mean) / rf_default_mean) * 100\n",
    "print(f\"ğŸ“ˆ Improvement: {rf_improvement:+.2f}%\")\n",
    "\n",
    "results_comparison.append(\n",
    "    {\n",
    "        \"Model\": \"Random Forest (Advanced)\",\n",
    "        \"Default Accuracy\": f\"{rf_default_mean:.4f}\",\n",
    "        \"Tuned Accuracy\": f\"{rf_grid.best_score_:.4f}\",\n",
    "        \"Improvement\": f\"{rf_improvement:+.2f}%\",\n",
    "        \"Best Params\": str(rf_grid.best_params_),\n",
    "        \"Tuning Time\": f\"{rf_tuning_time:.1f}s\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda2b81",
   "metadata": {},
   "source": [
    "## 2. Decision Tree Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6627cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³\n",
      "2. DECISION TREE CLASSIFIER - ADVANCED TUNING\n",
      "ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³ğŸŒ³\n",
      "\n",
      "ğŸ“Œ Default Parameters Performance:\n",
      "   Accuracy: 0.9163 (+/- 0.0142)\n",
      "   Training Time: 0.17s\n",
      "\n",
      "ğŸ”§ Advanced Hyperparameter Tuning...\n",
      "   Total combinations: 28224 = 28,224\n",
      "Fitting 5 folds for each of 28224 candidates, totalling 141120 fits\n",
      "   Accuracy: 0.9163 (+/- 0.0142)\n",
      "   Training Time: 0.17s\n",
      "\n",
      "ğŸ”§ Advanced Hyperparameter Tuning...\n",
      "   Total combinations: 28224 = 28,224\n",
      "Fitting 5 folds for each of 28224 candidates, totalling 141120 fits\n",
      "\n",
      "âœ“ Tuning completed in 413.27s\n",
      "âœ“ Best Parameters: {'ccp_alpha': 0.0, 'criterion': 'gini', 'max_depth': 15, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 3, 'splitter': 'random'}\n",
      "âœ“ Best CV Score: 0.9372\n",
      "ğŸ“ˆ Improvement: +2.28%\n",
      "\n",
      "âœ“ Tuning completed in 413.27s\n",
      "âœ“ Best Parameters: {'ccp_alpha': 0.0, 'criterion': 'gini', 'max_depth': 15, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 3, 'splitter': 'random'}\n",
      "âœ“ Best CV Score: 0.9372\n",
      "ğŸ“ˆ Improvement: +2.28%\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸŒ³\" * 40)\n",
    "print(\"2. DECISION TREE CLASSIFIER - ADVANCED TUNING\")\n",
    "print(\"ğŸŒ³\" * 40)\n",
    "\n",
    "# Default performance\n",
    "print(\"\\nğŸ“Œ Default Parameters Performance:\")\n",
    "dt_default = DecisionTreeClassifier(random_state=42)\n",
    "dt_default_start = time.time()\n",
    "dt_default_scores = cross_val_score(\n",
    "    dt_default, X_resampled, y_resampled, cv=cv, scoring=\"accuracy\"\n",
    ")\n",
    "dt_default_time = time.time() - dt_default_start\n",
    "dt_default_mean = dt_default_scores.mean()\n",
    "\n",
    "print(f\"   Accuracy: {dt_default_mean:.4f} (+/- {dt_default_scores.std():.4f})\")\n",
    "print(f\"   Training Time: {dt_default_time:.2f}s\")\n",
    "\n",
    "# Advanced Tuning - Expanded Grid\n",
    "print(\"\\nğŸ”§ Advanced Hyperparameter Tuning...\")\n",
    "dt_param_grid = {\n",
    "    \"max_depth\": [5, 10, 15, 20, 25, 30, 35, None],\n",
    "    \"min_samples_split\": [2, 3, 5, 7, 10, 15, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 3, 4, 5, 6, 8],\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"splitter\": [\"best\", \"random\"],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"ccp_alpha\": [0.0, 0.001, 0.01, 0.05],\n",
    "}\n",
    "\n",
    "print(f\"   Total combinations: {8*7*7*3*2*3*4} = 28,224\")\n",
    "\n",
    "dt_grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    dt_param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "dt_tuning_start = time.time()\n",
    "dt_grid.fit(X_resampled, y_resampled)\n",
    "dt_tuning_time = time.time() - dt_tuning_start\n",
    "\n",
    "print(f\"\\nâœ“ Tuning completed in {dt_tuning_time:.2f}s\")\n",
    "print(f\"âœ“ Best Parameters: {dt_grid.best_params_}\")\n",
    "print(f\"âœ“ Best CV Score: {dt_grid.best_score_:.4f}\")\n",
    "\n",
    "dt_improvement = ((dt_grid.best_score_ - dt_default_mean) / dt_default_mean) * 100\n",
    "print(f\"ğŸ“ˆ Improvement: {dt_improvement:+.2f}%\")\n",
    "\n",
    "results_comparison.append(\n",
    "    {\n",
    "        \"Model\": \"Decision Tree (Advanced)\",\n",
    "        \"Default Accuracy\": f\"{dt_default_mean:.4f}\",\n",
    "        \"Tuned Accuracy\": f\"{dt_grid.best_score_:.4f}\",\n",
    "        \"Improvement\": f\"{dt_improvement:+.2f}%\",\n",
    "        \"Best Params\": str(dt_grid.best_params_),\n",
    "        \"Tuning Time\": f\"{dt_tuning_time:.1f}s\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29254c11",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "856e9b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“Š\n",
      "3. LOGISTIC REGRESSION - ADVANCED TUNING\n",
      "ğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“ŠğŸ“Š\n",
      "\n",
      "ğŸ“Œ Default Parameters Performance:\n",
      "   Accuracy: 0.7814 (+/- 0.0192)\n",
      "   Training Time: 0.13s\n",
      "\n",
      "ğŸ”§ Advanced Hyperparameter Tuning...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "\n",
      "âœ“ Tuning completed in 2.32s\n",
      "âœ“ Best Parameters: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'l1_ratio': 0.9, 'class_weight': None, 'C': 1}\n",
      "âœ“ Best CV Score: 0.7822\n",
      "ğŸ“ˆ Improvement: +0.11%\n",
      "\n",
      "âœ“ Tuning completed in 2.32s\n",
      "âœ“ Best Parameters: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'l1_ratio': 0.9, 'class_weight': None, 'C': 1}\n",
      "âœ“ Best CV Score: 0.7822\n",
      "ğŸ“ˆ Improvement: +0.11%\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š\" * 40)\n",
    "print(\"3. LOGISTIC REGRESSION - ADVANCED TUNING\")\n",
    "print(\"ğŸ“Š\" * 40)\n",
    "\n",
    "# Default performance\n",
    "print(\"\\nğŸ“Œ Default Parameters Performance:\")\n",
    "lr_default = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_default_start = time.time()\n",
    "lr_default_scores = cross_val_score(\n",
    "    lr_default, X_resampled, y_resampled, cv=cv, scoring=\"accuracy\"\n",
    ")\n",
    "lr_default_time = time.time() - lr_default_start\n",
    "lr_default_mean = lr_default_scores.mean()\n",
    "\n",
    "print(f\"   Accuracy: {lr_default_mean:.4f} (+/- {lr_default_scores.std():.4f})\")\n",
    "print(f\"   Training Time: {lr_default_time:.2f}s\")\n",
    "\n",
    "# Advanced Tuning - Expanded Grid\n",
    "print(\"\\nğŸ”§ Advanced Hyperparameter Tuning...\")\n",
    "lr_param_grid = {\n",
    "    \"C\": [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 500],\n",
    "    \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"saga\", \"newton-cg\"],\n",
    "    \"max_iter\": [500, 1000, 1500, 2000, 3000],\n",
    "    \"class_weight\": [None, \"balanced\", {0: 1, 1: 2}, {0: 1, 1: 3}],\n",
    "    \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "}\n",
    "\n",
    "lr_random = RandomizedSearchCV(\n",
    "    LogisticRegression(random_state=42),\n",
    "    lr_param_grid,\n",
    "    n_iter=100,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "lr_tuning_start = time.time()\n",
    "lr_random.fit(X_resampled, y_resampled)\n",
    "lr_tuning_time = time.time() - lr_tuning_start\n",
    "\n",
    "print(f\"\\nâœ“ Tuning completed in {lr_tuning_time:.2f}s\")\n",
    "print(f\"âœ“ Best Parameters: {lr_random.best_params_}\")\n",
    "print(f\"âœ“ Best CV Score: {lr_random.best_score_:.4f}\")\n",
    "\n",
    "lr_improvement = ((lr_random.best_score_ - lr_default_mean) / lr_default_mean) * 100\n",
    "print(f\"ğŸ“ˆ Improvement: {lr_improvement:+.2f}%\")\n",
    "\n",
    "results_comparison.append(\n",
    "    {\n",
    "        \"Model\": \"Logistic Regression (Advanced)\",\n",
    "        \"Default Accuracy\": f\"{lr_default_mean:.4f}\",\n",
    "        \"Tuned Accuracy\": f\"{lr_random.best_score_:.4f}\",\n",
    "        \"Improvement\": f\"{lr_improvement:+.2f}%\",\n",
    "        \"Best Params\": str(lr_random.best_params_),\n",
    "        \"Tuning Time\": f\"{lr_tuning_time:.1f}s\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ab595",
   "metadata": {},
   "source": [
    "## 4. SVM Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "756dc27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡\n",
      "4. SUPPORT VECTOR MACHINE - ADVANCED TUNING\n",
      "âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡âš¡\n",
      "\n",
      "ğŸ“Œ Default Parameters Performance:\n",
      "   Accuracy: 0.8752 (+/- 0.0210)\n",
      "   Training Time: 0.51s\n",
      "\n",
      "ğŸ”§ Advanced Hyperparameter Tuning...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "   Accuracy: 0.8752 (+/- 0.0210)\n",
      "   Training Time: 0.51s\n",
      "\n",
      "ğŸ”§ Advanced Hyperparameter Tuning...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "\n",
      "âœ“ Tuning completed in 462.97s\n",
      "âœ“ Best Parameters: {'shrinking': False, 'kernel': 'rbf', 'gamma': 1, 'degree': 3, 'class_weight': None, 'cache_size': 500, 'C': 1}\n",
      "âœ“ Best CV Score: 0.9950\n",
      "ğŸ“ˆ Improvement: +13.69%\n",
      "\n",
      "âœ“ Tuning completed in 462.97s\n",
      "âœ“ Best Parameters: {'shrinking': False, 'kernel': 'rbf', 'gamma': 1, 'degree': 3, 'class_weight': None, 'cache_size': 500, 'C': 1}\n",
      "âœ“ Best CV Score: 0.9950\n",
      "ğŸ“ˆ Improvement: +13.69%\n"
     ]
    }
   ],
   "source": [
    "print(\"âš¡\" * 40)\n",
    "print(\"4. SUPPORT VECTOR MACHINE - ADVANCED TUNING\")\n",
    "print(\"âš¡\" * 40)\n",
    "\n",
    "# Default performance\n",
    "print(\"\\nğŸ“Œ Default Parameters Performance:\")\n",
    "svm_default = SVC(random_state=42)\n",
    "svm_default_start = time.time()\n",
    "svm_default_scores = cross_val_score(\n",
    "    svm_default, X_resampled, y_resampled, cv=cv, scoring=\"accuracy\"\n",
    ")\n",
    "svm_default_time = time.time() - svm_default_start\n",
    "svm_default_mean = svm_default_scores.mean()\n",
    "\n",
    "print(f\"   Accuracy: {svm_default_mean:.4f} (+/- {svm_default_scores.std():.4f})\")\n",
    "print(f\"   Training Time: {svm_default_time:.2f}s\")\n",
    "\n",
    "# Advanced Tuning - Expanded Grid\n",
    "print(\"\\nğŸ”§ Advanced Hyperparameter Tuning...\")\n",
    "svm_param_grid = {\n",
    "    \"C\": [0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000],\n",
    "    \"gamma\": [\"scale\", \"auto\", 0.0001, 0.001, 0.01, 0.1, 0.5, 1],\n",
    "    \"kernel\": [\"rbf\", \"poly\", \"sigmoid\", \"linear\"],\n",
    "    \"degree\": [2, 3, 4, 5],\n",
    "    \"class_weight\": [None, \"balanced\", {0: 1, 1: 2}, {0: 1, 1: 3}],\n",
    "    \"shrinking\": [True, False],\n",
    "    \"cache_size\": [500, 1000],\n",
    "}\n",
    "\n",
    "svm_random = RandomizedSearchCV(\n",
    "    SVC(random_state=42),\n",
    "    svm_param_grid,\n",
    "    n_iter=100,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "svm_tuning_start = time.time()\n",
    "svm_random.fit(X_resampled, y_resampled)\n",
    "svm_tuning_time = time.time() - svm_tuning_start\n",
    "\n",
    "print(f\"\\nâœ“ Tuning completed in {svm_tuning_time:.2f}s\")\n",
    "print(f\"âœ“ Best Parameters: {svm_random.best_params_}\")\n",
    "print(f\"âœ“ Best CV Score: {svm_random.best_score_:.4f}\")\n",
    "\n",
    "svm_improvement = ((svm_random.best_score_ - svm_default_mean) / svm_default_mean) * 100\n",
    "print(f\"ğŸ“ˆ Improvement: {svm_improvement:+.2f}%\")\n",
    "\n",
    "results_comparison.append(\n",
    "    {\n",
    "        \"Model\": \"SVM (Advanced)\",\n",
    "        \"Default Accuracy\": f\"{svm_default_mean:.4f}\",\n",
    "        \"Tuned Accuracy\": f\"{svm_random.best_score_:.4f}\",\n",
    "        \"Improvement\": f\"{svm_improvement:+.2f}%\",\n",
    "        \"Best Params\": str(svm_random.best_params_),\n",
    "        \"Tuning Time\": f\"{svm_tuning_time:.1f}s\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c18f67",
   "metadata": {},
   "source": [
    "## 5. KNN Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db17f16",
   "metadata": {},
   "source": [
    "## 6. Neural Networks Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec9ffbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
      "6. NEURAL NETWORKS - ADVANCED ARCHITECTURE SEARCH\n",
      "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
      "\n",
      "ğŸ“Œ Testing advanced neural network architectures...\n",
      "\n",
      "ğŸ”§ Architecture 1: Improved Sigmoid + Heavy Dropout + LR Scheduler\n",
      "\n",
      "ğŸ“Œ Testing advanced neural network architectures...\n",
      "\n",
      "ğŸ”§ Architecture 1: Improved Sigmoid + Heavy Dropout + LR Scheduler\n",
      "   Accuracy: 0.7741\n",
      "\n",
      "ğŸ”§ Architecture 2: Deep RELU + BatchNorm + Regularization\n",
      "   Accuracy: 0.7741\n",
      "\n",
      "ğŸ”§ Architecture 2: Deep RELU + BatchNorm + Regularization\n",
      "   Accuracy: 0.8787\n",
      "\n",
      "ğŸ”§ Architecture 3: Leaky RELU + Advanced Regularization\n",
      "   Accuracy: 0.8787\n",
      "\n",
      "ğŸ”§ Architecture 3: Leaky RELU + Advanced Regularization\n",
      "   Accuracy: 0.7197\n",
      "\n",
      "ğŸ”§ Architecture 4: Wide & Deep Network (512-256-128-64)\n",
      "   Accuracy: 0.7197\n",
      "\n",
      "ğŸ”§ Architecture 4: Wide & Deep Network (512-256-128-64)\n",
      "   Accuracy: 0.7448\n",
      "\n",
      "ğŸ”§ Architecture 5: Optimized Leaky RELU + Small Batch Training\n",
      "   Accuracy: 0.7448\n",
      "\n",
      "ğŸ”§ Architecture 5: Optimized Leaky RELU + Small Batch Training\n",
      "   Accuracy: 0.7866\n",
      "\n",
      "âœ“ Best Neural Network: Deep RELU + BatchNorm\n",
      "âœ“ Best Accuracy: 0.8787\n",
      "\n",
      "ğŸ“ˆ Improvement over default RELU: +25.52%\n",
      "âœ“ Best neural network saved to: ../trained-models/nn_tuned_advanced.keras\n",
      "   Accuracy: 0.7866\n",
      "\n",
      "âœ“ Best Neural Network: Deep RELU + BatchNorm\n",
      "âœ“ Best Accuracy: 0.8787\n",
      "\n",
      "ğŸ“ˆ Improvement over default RELU: +25.52%\n",
      "âœ“ Best neural network saved to: ../trained-models/nn_tuned_advanced.keras\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ§ \" * 40)\n",
    "print(\"6. NEURAL NETWORKS - ADVANCED ARCHITECTURE SEARCH\")\n",
    "print(\"ğŸ§ \" * 40)\n",
    "\n",
    "try:\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Split data for neural network\n",
    "    X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(\n",
    "        X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    "    )\n",
    "\n",
    "    print(\"\\nğŸ“Œ Testing advanced neural network architectures...\")\n",
    "\n",
    "    nn_architectures = []\n",
    "\n",
    "    # Architecture 1: Improved Sigmoid with Heavy Dropout\n",
    "    print(\"\\nğŸ”§ Architecture 1: Improved Sigmoid + Heavy Dropout + LR Scheduler\")\n",
    "    model1 = Sequential(\n",
    "        [\n",
    "            Dense(256, activation=\"sigmoid\", input_shape=(X_train_nn.shape[1],)),\n",
    "            Dropout(0.4),\n",
    "            Dense(128, activation=\"sigmoid\"),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation=\"sigmoid\"),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation=\"sigmoid\"),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    model1.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=15, restore_best_weights=True\n",
    "    )\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5, min_lr=0.00001\n",
    "    )\n",
    "\n",
    "    history1 = model1.fit(\n",
    "        X_train_nn,\n",
    "        y_train_nn,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    _, acc1 = model1.evaluate(X_test_nn, y_test_nn, verbose=0)\n",
    "    print(f\"   Accuracy: {acc1:.4f}\")\n",
    "    nn_architectures.append((\"Sigmoid + Heavy Dropout\", acc1, model1))\n",
    "\n",
    "    # Architecture 2: RELU with BatchNorm + Residual-like\n",
    "    print(\"\\nğŸ”§ Architecture 2: Deep RELU + BatchNorm + Regularization\")\n",
    "    model2 = Sequential(\n",
    "        [\n",
    "            Dense(512, activation=\"relu\", input_shape=(X_train_nn.shape[1],)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.4),\n",
    "            Dense(256, activation=\"relu\"),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation=\"relu\"),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    model2.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    history2 = model2.fit(\n",
    "        X_train_nn,\n",
    "        y_train_nn,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    _, acc2 = model2.evaluate(X_test_nn, y_test_nn, verbose=0)\n",
    "    print(f\"   Accuracy: {acc2:.4f}\")\n",
    "    nn_architectures.append((\"Deep RELU + BatchNorm\", acc2, model2))\n",
    "\n",
    "    # Architecture 3: Leaky RELU (fixes dying neurons)\n",
    "    print(\"\\nğŸ”§ Architecture 3: Leaky RELU + Advanced Regularization\")\n",
    "    model3 = Sequential(\n",
    "        [\n",
    "            Dense(384, input_shape=(X_train_nn.shape[1],)),\n",
    "            LeakyReLU(alpha=0.1),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.4),\n",
    "            Dense(192),\n",
    "            LeakyReLU(alpha=0.1),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(96),\n",
    "            LeakyReLU(alpha=0.1),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(48),\n",
    "            LeakyReLU(alpha=0.1),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    model3.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    history3 = model3.fit(\n",
    "        X_train_nn,\n",
    "        y_train_nn,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    _, acc3 = model3.evaluate(X_test_nn, y_test_nn, verbose=0)\n",
    "    print(f\"   Accuracy: {acc3:.4f}\")\n",
    "    nn_architectures.append((\"Leaky RELU + Advanced\", acc3, model3))\n",
    "\n",
    "    # Architecture 4: Wide & Deep Network\n",
    "    print(\"\\nğŸ”§ Architecture 4: Wide & Deep Network (512-256-128-64)\")\n",
    "    model4 = Sequential(\n",
    "        [\n",
    "            Dense(512, activation=\"relu\", input_shape=(X_train_nn.shape[1],)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.5),\n",
    "            Dense(256, activation=\"relu\"),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.4),\n",
    "            Dense(128, activation=\"relu\"),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    model4.compile(\n",
    "        optimizer=Adam(learning_rate=0.0003),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    history4 = model4.fit(\n",
    "        X_train_nn,\n",
    "        y_train_nn,\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    _, acc4 = model4.evaluate(X_test_nn, y_test_nn, verbose=0)\n",
    "    print(f\"   Accuracy: {acc4:.4f}\")\n",
    "    nn_architectures.append((\"Wide & Deep\", acc4, model4))\n",
    "\n",
    "    # Architecture 5: Optimized Leaky RELU with Smaller Batch\n",
    "    print(\"\\nğŸ”§ Architecture 5: Optimized Leaky RELU + Small Batch Training\")\n",
    "    model5 = Sequential(\n",
    "        [\n",
    "            Dense(320, input_shape=(X_train_nn.shape[1],)),\n",
    "            LeakyReLU(alpha=0.2),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.5),\n",
    "            Dense(160),\n",
    "            LeakyReLU(alpha=0.2),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.4),\n",
    "            Dense(80),\n",
    "            LeakyReLU(alpha=0.2),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(40),\n",
    "            LeakyReLU(alpha=0.2),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    model5.compile(\n",
    "        optimizer=Adam(learning_rate=0.0008),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    history5 = model5.fit(\n",
    "        X_train_nn,\n",
    "        y_train_nn,\n",
    "        epochs=100,\n",
    "        batch_size=16,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    _, acc5 = model5.evaluate(X_test_nn, y_test_nn, verbose=0)\n",
    "    print(f\"   Accuracy: {acc5:.4f}\")\n",
    "    nn_architectures.append((\"Optimized Leaky RELU\", acc5, model5))\n",
    "\n",
    "    # Find best neural network\n",
    "    best_nn = max(nn_architectures, key=lambda x: x[1])\n",
    "\n",
    "    print(f\"\\nâœ“ Best Neural Network: {best_nn[0]}\")\n",
    "    print(f\"âœ“ Best Accuracy: {best_nn[1]:.4f}\")\n",
    "\n",
    "    # Compare with default (70% RELU from original)\n",
    "    nn_default_acc = 0.70\n",
    "    nn_improvement = ((best_nn[1] - nn_default_acc) / nn_default_acc) * 100\n",
    "\n",
    "    results_comparison.append(\n",
    "        {\n",
    "            \"Model\": f\"Neural Network ({best_nn[0]})\",\n",
    "            \"Default Accuracy\": f\"{nn_default_acc:.4f}\",\n",
    "            \"Tuned Accuracy\": f\"{best_nn[1]:.4f}\",\n",
    "            \"Improvement\": f\"{nn_improvement:+.2f}%\",\n",
    "            \"Best Params\": f\"{best_nn[0]}, BatchNorm, Dropout, EarlyStopping, LR Scheduler\",\n",
    "            \"Tuning Time\": \"N/A (architecture search)\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"\\nğŸ“ˆ Improvement over default RELU: {nn_improvement:+.2f}%\")\n",
    "\n",
    "    # Save best neural network model\n",
    "    best_model = best_nn[2]\n",
    "    best_model.save(\"../trained-models/nn_tuned_advanced.keras\")\n",
    "    print(f\"âœ“ Best neural network saved to: ../trained-models/nn_tuned_advanced.keras\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"\\nâš ï¸  TensorFlow not available: {e}\")\n",
    "    print(\"Skipping neural network tuning...\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error during neural network tuning: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "849e0aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯\n",
      "5. K-NEAREST NEIGHBORS - ADVANCED TUNING\n",
      "ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯\n",
      "\n",
      "ğŸ“Œ Default Parameters Performance:\n",
      "   Accuracy: 0.8174 (+/- 0.0170)\n",
      "   Training Time: 0.15s\n",
      "\n",
      "ğŸ”§ Advanced Hyperparameter Tuning...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "   Accuracy: 0.8174 (+/- 0.0170)\n",
      "   Training Time: 0.15s\n",
      "\n",
      "ğŸ”§ Advanced Hyperparameter Tuning...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "\n",
      "âœ“ Tuning completed in 5.02s\n",
      "âœ“ Best Parameters: {'weights': 'distance', 'p': 4, 'n_neighbors': 3, 'metric': 'hamming', 'leaf_size': 50, 'algorithm': 'ball_tree'}\n",
      "âœ“ Best CV Score: 0.9121\n",
      "ğŸ“ˆ Improvement: +11.58%\n",
      "\n",
      "âœ“ Tuning completed in 5.02s\n",
      "âœ“ Best Parameters: {'weights': 'distance', 'p': 4, 'n_neighbors': 3, 'metric': 'hamming', 'leaf_size': 50, 'algorithm': 'ball_tree'}\n",
      "âœ“ Best CV Score: 0.9121\n",
      "ğŸ“ˆ Improvement: +11.58%\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ¯\" * 40)\n",
    "print(\"5. K-NEAREST NEIGHBORS - ADVANCED TUNING\")\n",
    "print(\"ğŸ¯\" * 40)\n",
    "\n",
    "# Default performance\n",
    "print(\"\\nğŸ“Œ Default Parameters Performance:\")\n",
    "knn_default = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_default_start = time.time()\n",
    "knn_default_scores = cross_val_score(\n",
    "    knn_default, X_resampled, y_resampled, cv=cv, scoring=\"accuracy\"\n",
    ")\n",
    "knn_default_time = time.time() - knn_default_start\n",
    "knn_default_mean = knn_default_scores.mean()\n",
    "\n",
    "print(f\"   Accuracy: {knn_default_mean:.4f} (+/- {knn_default_scores.std():.4f})\")\n",
    "print(f\"   Training Time: {knn_default_time:.2f}s\")\n",
    "\n",
    "# Advanced Tuning - Expanded Grid\n",
    "print(\"\\nğŸ”§ Advanced Hyperparameter Tuning...\")\n",
    "knn_param_grid = {\n",
    "    \"n_neighbors\": [3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 25, 31],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\", \"chebyshev\", \"hamming\"],\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "    \"leaf_size\": [10, 20, 30, 40, 50, 60],\n",
    "    \"p\": [1, 2, 3, 4],\n",
    "}\n",
    "\n",
    "knn_random = RandomizedSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    knn_param_grid,\n",
    "    n_iter=100,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "knn_tuning_start = time.time()\n",
    "knn_random.fit(X_resampled, y_resampled)\n",
    "knn_tuning_time = time.time() - knn_tuning_start\n",
    "\n",
    "print(f\"\\nâœ“ Tuning completed in {knn_tuning_time:.2f}s\")\n",
    "print(f\"âœ“ Best Parameters: {knn_random.best_params_}\")\n",
    "print(f\"âœ“ Best CV Score: {knn_random.best_score_:.4f}\")\n",
    "\n",
    "knn_improvement = ((knn_random.best_score_ - knn_default_mean) / knn_default_mean) * 100\n",
    "print(f\"ğŸ“ˆ Improvement: {knn_improvement:+.2f}%\")\n",
    "\n",
    "results_comparison.append(\n",
    "    {\n",
    "        \"Model\": \"KNN (Advanced)\",\n",
    "        \"Default Accuracy\": f\"{knn_default_mean:.4f}\",\n",
    "        \"Tuned Accuracy\": f\"{knn_random.best_score_:.4f}\",\n",
    "        \"Improvement\": f\"{knn_improvement:+.2f}%\",\n",
    "        \"Best Params\": str(knn_random.best_params_),\n",
    "        \"Tuning Time\": f\"{knn_tuning_time:.1f}s\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9242a5ae",
   "metadata": {},
   "source": [
    "## 7. Save Results and Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37e6433f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TUNING RESULTS SUMMARY\n",
      "================================================================================\n",
      "                                 Model Default Accuracy Tuned Accuracy Improvement                                                                                                                                            Best Params               Tuning Time\n",
      "              Random Forest (Advanced)           0.9632         0.9816      +1.91% {'bootstrap': False, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}                   7970.0s\n",
      "              Decision Tree (Advanced)           0.9163         0.9372      +2.28%    {'ccp_alpha': 0.0, 'criterion': 'gini', 'max_depth': 15, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 3, 'splitter': 'random'}                    413.3s\n",
      "        Logistic Regression (Advanced)           0.7814         0.7822      +0.11%                                                    {'solver': 'saga', 'penalty': 'l1', 'max_iter': 500, 'l1_ratio': 0.9, 'class_weight': None, 'C': 1}                      2.3s\n",
      "                        SVM (Advanced)           0.8752         0.9950     +13.69%                                        {'shrinking': False, 'kernel': 'rbf', 'gamma': 1, 'degree': 3, 'class_weight': None, 'cache_size': 500, 'C': 1}                    463.0s\n",
      "Neural Network (Deep RELU + BatchNorm)           0.7000         0.8787     +25.52%                                                                                 Deep RELU + BatchNorm, BatchNorm, Dropout, EarlyStopping, LR Scheduler N/A (architecture search)\n",
      "                        KNN (Advanced)           0.8174         0.9121     +11.58%                                      {'weights': 'distance', 'p': 4, 'n_neighbors': 3, 'metric': 'hamming', 'leaf_size': 50, 'algorithm': 'ball_tree'}                      5.0s\n",
      "\n",
      "âœ“ Results saved to: ../trained-models/tuning_results.csv\n",
      "âœ“ All tuned models saved successfully!\n",
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results_comparison)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TUNING RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(\"../trained-models/tuning_results.csv\", index=False)\n",
    "print(\"\\nâœ“ Results saved to: ../trained-models/tuning_results.csv\")\n",
    "\n",
    "# Save tuned models\n",
    "with open(\"../trained-models/rf_tuned_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf_grid.best_estimator_, f)\n",
    "\n",
    "with open(\"../trained-models/dt_tuned_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dt_grid.best_estimator_, f)\n",
    "\n",
    "with open(\"../trained-models/lr_tuned_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lr_random.best_estimator_, f)\n",
    "\n",
    "with open(\"../trained-models/svm_tuned_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(svm_random.best_estimator_, f)\n",
    "\n",
    "with open(\"../trained-models/knn_tuned_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(knn_random.best_estimator_, f)\n",
    "\n",
    "print(\"âœ“ All tuned models saved successfully!\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HYPERPARAMETER TUNING COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
